# Conceitos bÃ¡sicos

## IntroduÃ§Ã£o

O [Apache Kafka](https://kafka.apache.org/) Ã© um sistema distribuÃ­do em software livre que implementa a arquitetura de fluxo de eventos, e que na prÃ¡tica permite a base tecnolÃ³gica para a criaÃ§Ã£o e integraÃ§Ã£o de sistemas de alta disponibilidade em tempo real.

A gama de casos de uso possÃ­veis Ã© bem vasta, porÃ©m bastante especÃ­fica. Certifique-se de que o Kafka Ã© a ferramenta correta para o seu problema antes de utilizÃ¡-lo.

Este material Ã© autoral, fruto de estudos informais. Refira-se Ã  [documentaÃ§Ã£o oficial](https://kafka.apache.org/documentation/) sempre que necessÃ¡rio e ao sentir algum estranhamento em relaÃ§Ã£o ao que foi escrito.

## Infraestrutura

Pode ser instalado no Windows ou Mac, mas preferencialmente em Linux. Consiste em dois softwares principais, o Kafka e o ZooKeeper, bem como diversas outras ferramentas de apoio.

- **Apache Kafka** faz toda a mÃ¡gica acontecer.
- **ZooKeeper** faz a gestÃ£o e coordenaÃ§Ã£o das instÃ¢ncias.

Ã‰ possÃ­vel utilizar clientes em praticamente todas as plataformas populares de desenvolvimento.

### _Brokers_ e _clusters_

Cada instÃ¢ncia de servidor Kafka Ã© chamada de _broker_. Ã‰ responsÃ¡vel por todas as aÃ§Ãµes de persistÃªncia e leitura.

Pode ser organizado em conjuntos de _brokers_ chamados _clusters_, idealmente com 3 ou mais instÃ¢ncias, podendo alcanÃ§ar 100 (ou mais) em grandes projetos.

Em um _cluster_, cada _broker_ Ã© identificado por um nÃºmero identificador inteiro. Os metadados sÃ£o conhecidos por todos os _brokers_ igualmente, de forma que nÃ£o hÃ¡ um endereÃ§o de _cluster_ e qualquer um deles ao ser endereÃ§ado atua como _bootstrap server_ provendo acesso ao _cluster_ como um todo. Nenhum _broker_ possui todos os dados, pois eles ficam distribuÃ­dos.

Pode-se conseguir alta disponibilidade utilizando replicaÃ§Ã£o. O fator de replicaÃ§Ã£o expressa quantas rÃ©plicas serÃ£o mantidas dos dados, em _brokers_ diferentes. Assim, o fator de replicaÃ§Ã£o varia entre 1 e o nÃºmero de _brokers_ do _cluster_. Um _cluster_ com `N` _brokers_ Ã© tolerante a falhas em `N-1` deles simultaneamente. Por exemplo, com 3 _brokers_ podemos ter um com falha, um em atualizaÃ§Ã£o e um terceiro mantendo o sistema disponÃ­vel.

A coordenaÃ§Ã£o Ã© feita automaticamente pelo ZooKeeper. Um dos _brokers_ serÃ¡ eleito o lÃ­der e serÃ¡ responsÃ¡vel por receber e entregar os dados. Os demais atuarÃ£o como rÃ©plicas, mantendo os dados sincronizados (_in-sync replica_ ou ISR). Essa gestÃ£o Ã© feita para cada partiÃ§Ã£o  em cada tÃ³pico, portanto a carga pode ser balanceada adequadamente.

AlÃ©m de gerenciar os _brokers_ em um _cluster_ e coordenar a lideranÃ§a de partiÃ§Ã£o, o ZooKeeper tambÃ©m notifica os _brokers_ sobre mudanÃ§as na estrutura, mantendo os metadados atualizados em todos os servidores. Com tantas atribuiÃ§Ãµes, o ZooKeeper Ã© obrigatÃ³rio, mesmo que haja somente um _broker_.

O prÃ³prio ZooKeeper deve ser preferencialmente mantido em um _cluster_, sempre com nÃºmero Ã­mpar de servidores. Eles elegem um lÃ­der, que trata as entradas (_write_), com os demais seguidores efetuando as saÃ­das (_read_).

O ZooKeeper Ã© transparente aos consumidores e produtores, e acessado somente pelo Kafka.

## Modelo de armazenamento

Podemos pensar no Kafka como um grande _log_, onde dados em fluxo sÃ£o armazenados em uma sequÃªncia temporal imutÃ¡vel, para serem consumidos ordenadamente. Dados de mesma natureza sÃ£o agrupados em _tÃ³picos_, e os tÃ³picos sÃ£o gravados em arquivos fÃ­sicos distribuÃ­dos entre os _brokers_ chamados _partiÃ§Ãµes_.

Os dados sÃ£o retidos por um tempo finito no Kafka (ex. 1 semana), portanto nÃ£o sÃ£o indefinidamente persistentes. Considere casos de uso de dados em movimento, e nÃ£o de dados em repouso.

### TÃ³picos

Os tÃ³picos sÃ£o agrupamentos de dados de mesma categoria. Atuam como tabelas em um banco relacional, porÃ©m sem as _constraints_. Outra diferenÃ§a importante Ã© a impossibilidade de alteraÃ§Ã£o dos dados: os dados sÃ£o imutÃ¡veis. Podem-se criar quantos tÃ³picos forem necessÃ¡rios, e cada tÃ³pico pode receber dados de mÃºltiplas origens e entregar dados para mÃºltiplos destinos.

Ao criar um tÃ³pico definimos um nome identificador, a quantidade de partiÃ§Ãµes desejadas, e a quantidade de rÃ©plicas que estarÃ£o disponÃ­veis.

Ao se produzir uma mensagem (ou seja, gravar um mensagem em um tÃ³pico) o Kafka verifica se o tÃ³pico existe (cria com a configuraÃ§Ã£o padrÃ£o se nÃ£o existir), lÃª os seus metadados com as partiÃ§Ãµes e configuraÃ§Ãµes de replicaÃ§Ã£o, e efetua a gravaÃ§Ã£o. ApÃ³s a gravaÃ§Ã£o, a mensagem estarÃ¡ disponÃ­vel para todos os consumidores interessados no tÃ³pico.

### PartiÃ§Ãµes

Os dados de um tÃ³pico sÃ£o gravados em arquivos fÃ­sicos de dados, chamados partiÃ§Ãµes. As partiÃ§Ãµes sÃ£o numeradas sequencialmente, a partir de `0`. Ter mais de uma partiÃ§Ã£o permite que os dados e a carga sejam distribuÃ­dos, aumentando a tolerÃ¢ncia a falhas e a disponibilidade.

Caso estejamos em um _cluster_, elas serÃ£o distribuÃ­das entre os _brokers_ disponÃ­veis, a critÃ©rio do Kafka.

Exemplo:

Em um _cluster_ com 3 _brokers_ `1`, `2` e `3`, sÃ£o criados os tÃ³picos `A` com 3 partiÃ§Ãµes, `B` com 4 partiÃ§Ãµes, `C` com 2 partiÃ§Ãµes, e `D` com 1 partiÃ§Ã£o.

- O tÃ³pico `A` terÃ¡ suas partiÃ§Ãµes divididas igualmente entre os _brokers_, possivelmente um em cada;
- O tÃ³pico `B` terÃ¡ suas partiÃ§Ãµes divididas igualmente entre os _brokers_ com um _broker_ que recebendo mais de uma partiÃ§Ã£o desse tÃ³pico;
- O tÃ³pico `C` terÃ¡ suas partiÃ§Ãµes divididas entre os _brokers_, com algum _broker_ nÃ£o recebendo nenhuma partiÃ§Ã£o;
- O tÃ³pico `D` terÃ¡ sua Ãºnica partiÃ§Ã£o alocada em um Ãºnico _broker_.

Uma possÃ­vel configuraÃ§Ã£o final seria:

- _Broker_ `1`
  - TÃ³pico `A` PartiÃ§Ã£o `1`
  - TÃ³pico `B` PartiÃ§Ã£o `2`
  - TÃ³pico `C` PartiÃ§Ã£o `0`
  - TÃ³pico `D` PartiÃ§Ã£o `0`
- _Broker_ `2`
  - TÃ³pico `A` PartiÃ§Ã£o `0`
  - TÃ³pico `B` PartiÃ§Ã£o `1`
  - TÃ³pico `B` PartiÃ§Ã£o `3`
- _Broker_ `3`
  - TÃ³pico `A` PartiÃ§Ã£o `2`
  - TÃ³pico `B` PartiÃ§Ã£o `0`
  - TÃ³pico `C` PartiÃ§Ã£o `1`

Em uma visÃ£o por tÃ³pico:

- TÃ³pico `A`
  - PartiÃ§Ã£o `0` no _Broker_ `2`
  - PartiÃ§Ã£o `1` no _Broker_ `1`
  - PartiÃ§Ã£o `2` no _Broker_ `3`
- TÃ³pico `B`
  - PartiÃ§Ã£o `0` no _Broker_ `3`
  - PartiÃ§Ã£o `1` no _Broker_ `2`
  - PartiÃ§Ã£o `2` no _Broker_ `1`
  - PartiÃ§Ã£o `3` no _Broker_ `2`
- TÃ³pico `C`
  - PartiÃ§Ã£o `0` no _Broker_ `1`
  - PartiÃ§Ã£o `1` no _Broker_ `3`
- TÃ³pico `D`
  - PartiÃ§Ã£o `0` no _Broker_ `1`

### ReplicaÃ§Ã£o

Em um tÃ³pico criado com o fator de replicaÃ§Ã£o padrÃ£o `1`, cada partiÃ§Ã£o contÃ©m dados distintos, de forma que cada dado estÃ¡ em uma e somente uma partiÃ§Ã£o. Se definirmos um nÃºmero maior de replicaÃ§Ã£o, haverÃ£o cÃ³pias fÃ­sicas da partiÃ§Ã£o (chamadas _in-sync replicas_ ou _ISRs_) distribuÃ­das necessariamente em _brokers_ diferentes, inativas e sincronizadas para assumir em caso de falha da partiÃ§Ã£o ativa (chamada de partiÃ§Ã£o lÃ­der).

O fator de replicaÃ§Ã£o, portanto, Ã© definido entre 1 e a quantidade de _brokers_ existentes no _cluster_.

Sempre haverÃ¡ uma partiÃ§Ã£o lÃ­der eleita entre as rÃ©plicas, que atenderÃ¡ toda a carga. As demais se manterÃ£o como cÃ³pias estÃ¡ticas sincronizadas, podendo assumir a lideranÃ§a eventualmente a critÃ©rio do Kafka.

### Dados, produtores e consumidores

Os sÃ£o mantidos nos tÃ³picos, apÃ³s derem recebidos como _payloads_ da mensagens enviadas pelas aplicaÃ§Ãµes com o papel de produtores, e sob demanda entregues Ã s aplicaÃ§Ãµes com o papel de consumidores.

Os dados sÃ£o armazenados e transportados em forma binÃ¡ria em _arrays_ de _bytes_. Assim, devemos tratar nos produtores e consumidores a serializaÃ§Ã£o dos dados apropriada ao caso de uso. As bibliotecas costumam incluir diversos serializadores para formatos comuns.

#### ProduÃ§Ã£o

Os produtores escrevem dados nos tÃ³picos. NÃ£o Ã© necessÃ¡rio saber qual a partiÃ§Ã£o ou qual _broker_ acessar, mas somente o endereÃ§o de um dos _brokers_ do _cluster_ e o nome do tÃ³pico. Toda a resoluÃ§Ã£o Ã© feita pelo Kafka, garantindo a estabilidade durante as indisponibilidades.

Os dados sÃ£o empacotados em registros ou mensagens contendo um cabeÃ§alho, uma chave opcional e o valor do dado propriamente dito. SÃ£o enviados em lotes com um ou mais registros, com seu prÃ³prio cabeÃ§alho, em um processo chamado _flush_.

O produtor pode solicitar trÃªs tipos de confirmaÃ§Ã£o de recebimento apÃ³s envio:

- `acks=0` nÃ£o aguarda confirmaÃ§Ã£o. Ã‰ mais rÃ¡pido, mas nÃ£o garante a entrega.
- `acks=1` aguarda confirmaÃ§Ã£o do lÃ­der, com possÃ­vel perda de dados em caso de falha no lÃ­der entre a confirmaÃ§Ã£o e a replicaÃ§Ã£o.
- `acks=all` aguarda o lÃ­der e a replicaÃ§Ã£o, portanto nÃ£o hÃ¡ perdas, ao custo de latÃªncia.

ğŸŒ A confirmaÃ§Ã£o `acks=all` ainda assim pode tolerar alguma indisponibilidade nas rÃ©plicas. Isso pode ser ajustado pela combinaÃ§Ã£o do fator de replicaÃ§Ã£o do tÃ³pico com a configuraÃ§Ã£o `min.insync.replicas` (quantidade mÃ­nima de _brokers_ - incluÃ­ndo o lÃ­der - que devem responder positivamente antes da confirmaÃ§Ã£o) no _broker_ ou no tÃ³pico. Por exemplo, com fator de replicaÃ§Ã£o 5 e mÃ­nimo _in sync_ de 3, a confirmaÃ§Ã£o virÃ¡ mesmo com dois _brokers_ fora, apesar do `acks=all`.

O Kafka decide em qual partiÃ§Ã£o o dado serÃ¡ gravado. A quantidade de mensagens jÃ¡ gravadas nÃ£o influencia na decisÃ£o, portanto nÃ£o hÃ¡ divisÃ£o igualitÃ¡ria de espaÃ§o ocupado ou de quantidade de dados armazenados.

O dado recebe um identificador Ãºnico incremental naquela partiÃ§Ã£o, chamado _offset_, independente das demais partiÃ§Ãµes. Ou seja, podemos ter o _offset_ `1` na partiÃ§Ã£o `0` e tambÃ©m na partiÃ§Ã£o `1`, porÃ©m com dados diferentes sem nenhuma relaÃ§Ã£o alÃ©m de estarem no mesmo tÃ³pico. A ordem cronolÃ³gica dos dados Ã© garantida dentro de uma partiÃ§Ã£o, ou seja, o dado de _offset_ `17` certamente chegou antes do dado de _offset_ `18`. PorÃ©m, nÃ£o Ã© possÃ­vel garantir ordenaÃ§Ã£o entre diferentes partiÃ§Ãµes, assim o dado de _offset_ `17` em uma partiÃ§Ã£o pode ser anterior ao o dado de _offset_ `5` em outra.

Exemplo:

Foram enviados os dados ğŸŒ, ğŸ¥‘, ğŸ‰, ğŸ“ e ğŸ‡ ao tÃ³pico `frutas` que possui duas partiÃ§Ãµes, nessa sequÃªncia. O Kafka decidiu armazenar da seguinte forma:

- PartiÃ§Ã£o `0` = [ğŸ‰, ğŸ“]
  - Offset `1` = ğŸ‰
  - Offset `2` = ğŸ“
- PartiÃ§Ã£o `1` = [ğŸŒ, ğŸ¥‘, ğŸ‡]
  - Offset `1` = ğŸŒ
  - Offset `2` = ğŸ¥‘
  - Offset `3` = ğŸ‡

Perceba que garantimos que ğŸ“ chegou ao tÃ³pico apÃ³s ğŸ‰, e que ğŸŒ chegou antes de ğŸ‡, mas nada podemos falar sobre a relaÃ§Ã£o temporal entre ğŸ‰ e ğŸ¥‘.

Qualquer outra sequÃªncia seria vÃ¡lida, desde que os _offsets_ na mesma partiÃ§Ã£o garantam a sequÃªncia interna.

##### Retentativas e produtores idempotentes

Em caso de exceÃ§Ãµes no envio, o erro pode ser tratado pelo desenvolvedor, ou automaticamente pelo produtor.

Os produtores podem usar a configuraÃ§Ã£o `retries` para fazer a retentativa automÃ¡tica, e esse Ã© inclusive o comportamento padrÃ£o do Kafka nas versÃµes >= 2.1. As configuraÃ§Ãµes importantes em relaÃ§Ã£o a retentativas automÃ¡ticas sÃ£o:

- `retries` indica quantas tentativas serÃ£o feitas em caso de exceÃ§Ã£o (`0` = nenhuma);
- `retry.backoff.ms` indica o tempo entre as retentativas;
- `delivery.timeout.ms` indica o limite de tempo para retentativas (padrÃ£o Ã© 2 minutos);
- `max.in.flight.requests.per.connection` indica o nÃºmero mÃ¡ximo de requisiÃ§Ãµes em paralelo provenientes de uma mesma conexÃ£o. Caso seja maior do que um (padrÃ£o Ã© `5`), pode gerar gravaÃ§Ãµes fora de ordem em uma retentativa. O valor `1` garante o sequenciamento dentro da mesma conexÃ£o, mas bloqueia o paralelismo.

A soluÃ§Ã£o mais simples para habilitar as retentativas e garantir a ordenaÃ§Ã£o dentro da conexÃ£o Ã© usar produtores idempotentes. Isso Ã© feito ativando a configuraÃ§Ã£o do produtor `enable.idempotence=true`. Isso exige `acks=all`, `max.in.flight.requests.per.connection=5` e `retries=Integer.MAX_VALUE` (ou seja, ou valores padrÃ£o). HÃ¡ uma explicaÃ§Ã£o detalhada do algoritmo usado [aqui](https://issues.apache.org/jira/browse/KAFKA-5494).

Em resumo, podemos criar um produtor seguro usando `enable.idempotence=true` associado a `min.insync.replicas=2` no tÃ³pico ou no _broker_.

##### Desempenho

Algo que melhoria substancialmente o desempenho, contraintuitivamente, Ã© a uso de compactaÃ§Ã£o na produÃ§Ã£o de mensagens, usando a configuraÃ§Ã£o `compression.type`. [Escolha um dos algoritmos de compactaÃ§Ã£o](https://blog.cloudflare.com/squeezing-the-firehose/), por exemplo `lz4`, `snappy` ou `zstd` e os lotes de mensagens serÃ£o compactados, reduzindo a latÃªncia, o trÃ¡fego e o espaÃ§o de armazenamento. Podemos controlar a formaÃ§Ã£o dos lotes usando `linger.ms` (intervalo a aguardar antes de enviar, permitindo assim a formaÃ§Ã£o de lotes maiores -- o padrÃ£o Ã© `0`) e `batch.size` (tamanho mÃ¡ximo em bytes de um lote -- o padrÃ£o Ã© 16KB).

Uma boa configuraÃ§Ã£o para comeÃ§ar a testar Ã© `compression.type=snappy`, `linger.ms=20` e `batch.size=32768` (32 * 1024 bytes = 32KB).

#### Consumo

Consumidores lÃªem dados de tÃ³picos. NÃ£o Ã© necessÃ¡rio saber qual a partiÃ§Ã£o ou qual _broker_ acessar, mas somente o endereÃ§o de um dos _brokers_ do _cluster_ e o nome do tÃ³pico. Toda a resoluÃ§Ã£o Ã© feita pelo Kafka, garantindo a estabilidade durante as indisponibilidades.

Os dados serÃ£o lidos ordenadamente dentro de cada partiÃ§Ã£o, porÃ©m as partiÃ§Ãµes serÃ£o tratadas paralelamente. Ou seja, nÃ£o hÃ¡ garantia de entrega na ordem em que os dados chegaram no tÃ³pico, mas somente dentro de cada partiÃ§Ã£o.

Por exemplo, considerando o tÃ³pico `frutas` no estado definido acima, um consumidor pode receber [ğŸŒ, ğŸ¥‘, ğŸ‰, ğŸ“, ğŸ‡] conforme a sequÃªncia enviada, porÃ©m seria uma coincidÃªncia. SÃ£o igualmente vÃ¡lidas e possÃ­vel quaisquer combinaÃ§Ãµes em que ğŸ‰ venha antes de ğŸ“, e que ğŸ‡ venha depois de ğŸ¥‘ que por sua vez venha depois de ğŸŒ.

Ilustrando casos vÃ¡lidos:

- [ğŸ‰, ğŸ“, ğŸŒ, ğŸ¥‘, ğŸ‡], em que se consumiu a partiÃ§Ã£o `0` e depois a `1`;
- [ğŸŒ, ğŸ¥‘, ğŸ‡, ğŸ‰, ğŸ“], em que se consumiu a partiÃ§Ã£o `1` e depois a `0`;
- [ğŸŒ, ğŸ¥‘, ğŸ‰, ğŸ“, ğŸ‡], em que se consumiu alternadamente, mantendo a sequencia original por coincidÃªncia;
- [ğŸŒ, ğŸ¥‘, ğŸ‰, ğŸ‡, ğŸ“], em que se consumiu alternadamente, nÃ£o mantendo a sequencia original mas mantendo a sequÃªncia entre partiÃ§Ãµes;
- [ğŸ‰, ğŸŒ, ğŸ¥‘, ğŸ“, ğŸ‡], como acima, mas em outra combinaÃ§Ã£o.

Os _offsets_ atuais de cada consumidor indicam o ponto atual de leitura de um consumidor em um tÃ³pico, por partiÃ§Ã£o, e permitem continuar do mesmo ponto ao retomar o consumo. Ficam armazenados no tÃ³pico `__consumer_offsets` e sÃ£o mantidos automaticamente. Na entrega o consumidor terÃ¡ o seu registro de _offsets_ atual alterado pelo Kafka, de forma que ele nÃ£o o receberÃ¡ em duplicidade, de acordo com a semÃ¢ntica de entrega estabelecida.

O consumidor pode utilizar uma entre trÃªs semÃ¢nticas de entrega:

- `at most once`: mensagens podem ser perdidas, mas nunca sÃ£o reenviadas em duplicidade. O _offset_ Ã© ajustado ao realizar a leitura, e em caso de erro no processamento das mensagens pelo consumidor, ao reiniciar as mensagem nÃ£o serÃ£o reentregues, pois a confirmaÃ§Ã£o jÃ¡ foi dada.
- `at least once`: mensagens nunca sÃ£o perdidas, mas podem ser reenviadas em duplicidade. O _offset_ Ã© ajustado somente ao final do processo, e em caso de erro na alteraÃ§Ã£o do _offset_ ou erro no processamento pelo consumidor, a entrega pode reiniciar de onde comeÃ§ou na Ãºltima vez, potencialmente repetindo dados jÃ¡ processados. Para implementar isso, faÃ§a o _commit_ de _offset_ manualmente, e garanta a idempotÃªncia do procedimento. Ã‰ o mÃ©todo preferido.
- `exactly once`: onde Ã© garantida a entrega uma e somente uma vez, porÃ©m Ã© restrita a processos internos do Kafka.

ğŸ’¡ Podemos implementar idempotÃªncia definindo chaves Ãºnicas para cada registro recebido ao consumir. Caso o dado nÃ£o possua uma chave Ãºnica intrÃ­nseca, uma maneira bem simples Ã© utilizar uma chave no mecanismo de persistÃªncia que combine o nome do tÃ³pico, a partiÃ§Ã£o e o _offset_, combinaÃ§Ã£o essa Ãºnica em uma instalaÃ§Ã£o Kafka.

O modelo de consumo Ã© o _poll_ (e nÃ£o _push_). Assim, em intervalos de tempo o consumidor requisita novos registros, em vez de ser estimulado pelo Kafka. Isso permite maior controle pelos consumidores da frequÃªncia e do volume desejados para receber os dados.

ğŸ±â€ğŸ‘¤ Podemos especificar o tamanho mÃ­nimo em bytes de um pacote a ser recebido usando `fetch.min.bytes` (o padrÃ£o Ã© `1`), criando lotes de transmissÃ£o e reduzindo o trÃ¡fego ao custo da latÃªncia (o mÃ¡ximo Ã© controlado por `fetch.max.bytes` com o padrÃ£o de 50MB). Podemos tambÃ©m controlar o tamanho mÃ¡ximo do lote de registros usando `max.poll.records` (o padrÃ£o Ã© `500`), aumentando a quantidade em caso de mensagens pequenas ou de alta quantidade de RAM disponÃ­vel. O tamanho mÃ¡ximo em bytes por partiÃ§Ã£o pode ser controlado por `max.partitions.fetch.bytes` (o padrÃ£o Ã© 1MB). SÃ³ altere essas configuraÃ§Ãµes em casos extremos de problemas de desempenho.

##### RetenÃ§Ã£o e repetiÃ§Ã£o

Espera-se que um consumidor (ou grupo de consumidores) faÃ§a leituras contÃ­nuas. Em caso de falha ou inatividade por um perÃ­odo prolongado (maior do que o perÃ­odo de retenÃ§Ã£o) seu _offset_ se tornarÃ¡ invÃ¡lido ou descartado.

Nesses casos, hÃ¡ trÃªs opÃ§Ãµes para o consumidor:

- com `auto.offset.reset=latest` serÃ£o lidos somente os novos dados, a partir da retomada do consumo.
- com `auto.offset.reset=earliest` serÃ£o lidos todos os dados disponÃ­veis novamente, desde o inÃ­cio.
- com `auto.offset.reset=none` serÃ¡ gerada uma exceÃ§Ã£o caso nÃ£o haja nenhum _offset_.

Pode-se ajustar o tempo de retenÃ§Ã£o por _broker_ usando `offset.retention.minutes` (o padrÃ£o Ã© uma semana).

Para repetir o consumo de um tÃ³pico (receber novamente os dados a partir de um _offset_) em um grupo de consumidores, use `kafka-consumers-groups` com a opÃ§Ã£o `--reset-offsets --execute --to-earliest` e reinicie os consumidores. Atente ao fato de que eles devem ser idempotentes.

AlÃ©m do _thread_ de _poll_ os consumidores em um grupo possuem um _thread_ de _heartbeat_ com o _broker_ coordenador do grupo. Ele serve para indicar que o consumidor ainda estÃ¡ ativo. Como os _threads_ sÃ£o independentes, a realizaÃ§Ã£o de _polls_ muito espaÃ§ados pode gerar problemas.

- `session.timeout.ms` (padrÃ£o 10s) indica o tempo mÃ¡ximo de espera entre _heartbeats_ antes que o consumidor seja considerado morto. Diminuir esse valor causarÃ¡ rebalanceamentos mais frequentes.
- `heartbeat.interval.ms` (padrÃ£o 3s) indica o tempo de espera entre envios de _heartbeats_ pelo consumidor. Ã‰ recomendado 1/3 do `timeout`.
- `max.poll.interval.ms` (padrÃ£o 5min) indica o mÃ¡ximo tempo decorrido entre dois _polls_ antes de declarar o consumidor morto. Esse tempo deve ser ajustado caso o tempo de processamento seja muito alto e nÃ£o possa ser reduzido.

#### Chaves em mensagens

Em alguns casos de uso podemos necessitar de algum controle sobre o ordenamento das mensagens. Para isso, podemos enviar junto aos dados uma chave. O Kafka garante que dados enviados com a mesma chave serÃ£o gravados na mesma partiÃ§Ã£o, desde que o nÃºmero de partiÃ§Ãµes se mantenha inalterado. Dessa maneira, temos a garantia do sequenciamento para mensagens com chaves semelhantes, jÃ¡ que estarÃ£o na mesma partiÃ§Ã£o. VocÃª ainda nÃ£o poderÃ¡ escolher em qual partiÃ§Ã£o a primeira mensagem daquela chave irÃ¡ ficar.

Um caso de uso comum seria o recebimento de posiÃ§Ãµes GPS de diversos veÃ­culos onde queremos garantir as leituras na sequÃªncia para cada um deles. PoderÃ­amos obter essa garantia enviado o identificador do veÃ­culo na chave, por exemplo. Isso forÃ§aria as leituras a ficarem na mesma partiÃ§Ã£o, onde a ordem Ã© garantida.

As chaves, assim como os dados, sÃ£o armazenados e transportados em forma binÃ¡ria em _arrays_ de _bytes_, e podem ser serializados e desserializados pelos clientes conforme a necessidade.

ğŸ¤¯ A seleÃ§Ã£o da partiÃ§Ã£o Ã© feita atravÃ©s do cÃ¡lculo do resto da divisÃ£o de um inteiro calculado atravÃ©s do [hash nÃ£o criptogrÃ¡fico Murmur2](https://en.wikipedia.org/wiki/MurmurHash) do valor da chave pelo nÃºmero de partiÃ§Ãµes disponÃ­veis. Isso claramente distribui as entradas de forma dependente da quantidade de partiÃ§Ãµes, de forma que a alteraÃ§Ã£o nessa quantidade gera uma distribuiÃ§Ã£o diferente nas prÃ³ximas gravaÃ§Ãµes.

#### Grupos de consumidores

Para conseguirmos paralelizar o consumo sem repetir a leitura de um dado entre as instÃ¢ncias consumidoras, precisamos criar uma afinidade entre elas. Fazemos isso criando grupos de consumidores, que nada mais sÃ£o do que indicadores de que eles compartilham o mesmo _offset_ em cada partiÃ§Ã£o.

Um grupo de consumidores Ã© definido por um nome, e representa geralmente um _cluster_ de consumidores de uma Ãºnica aplicaÃ§Ã£o.

Em um grupo, uma partiÃ§Ã£o sempre serÃ¡ lida pelo mesmo consumidor, garantindo a ordenaÃ§Ã£o. Essa coordenaÃ§Ã£o Ã© feita automaticamente pelo Kafka.

Caso hajam mais consumidores do que partiÃ§Ãµes, eles ficarÃ£o inativos. Ainda assim podem ser Ãºteis, pois serÃ£o acionados assim que um dos consumidores fique indisponÃ­vel.
