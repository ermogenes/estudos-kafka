# Conceitos b√°sicos

## Introdu√ß√£o

O [Apache Kafka](https://kafka.apache.org/) √© um sistema distribu√≠do em software livre que implementa a arquitetura de fluxo de eventos, e que na pr√°tica permite a base tecnol√≥gica para a cria√ß√£o e integra√ß√£o de sistemas de alta disponibilidade em tempo real.

A gama de casos de uso poss√≠veis √© bem vasta, por√©m bastante espec√≠fica. Certifique-se de que o Kafka √© a ferramenta correta para o seu problema antes de utiliz√°-lo.

Este material √© autoral, fruto de estudos informais. Refira-se √† [documenta√ß√£o oficial](https://kafka.apache.org/documentation/) sempre que necess√°rio e ao sentir algum estranhamento em rela√ß√£o ao que foi escrito.

## Infraestrutura

Pode ser instalado no Windows ou Mac, mas preferencialmente em Linux. Consiste em dois softwares principais, o Kafka e o ZooKeeper, bem como diversas outras ferramentas de apoio.

- **Apache Kafka** faz toda a m√°gica acontecer.
- **ZooKeeper** faz a gest√£o e coordena√ß√£o das inst√¢ncias.

√â poss√≠vel utilizar clientes em praticamente todas as plataformas populares de desenvolvimento.

### _Brokers_ e _clusters_

Cada inst√¢ncia de servidor Kafka √© chamada de _broker_. √â respons√°vel por todas as a√ß√µes de persist√™ncia e leitura.

Pode ser organizado em conjuntos de _brokers_ chamados _clusters_, idealmente com 3 ou mais inst√¢ncias, podendo alcan√ßar 100 (ou mais) em grandes projetos.

Em um _cluster_, cada _broker_ √© identificado por um n√∫mero identificador inteiro. Os metadados s√£o conhecidos por todos os _brokers_ igualmente, de forma que n√£o h√° um endere√ßo de _cluster_ e qualquer um deles ao ser endere√ßado atua como _bootstrap server_ provendo acesso ao _cluster_ como um todo. Nenhum _broker_ possui todos os dados, pois eles ficam distribu√≠dos.

Pode-se conseguir alta disponibilidade utilizando replica√ß√£o. O fator de replica√ß√£o expressa quantas r√©plicas ser√£o mantidas dos dados, em _brokers_ diferentes. Assim, o fator de replica√ß√£o varia entre 1 e o n√∫mero de _brokers_ do _cluster_. Um _cluster_ com `N` _brokers_ √© tolerante a falhas em `N-1` deles simultaneamente. Por exemplo, com 3 _brokers_ podemos ter um com falha, um em atualiza√ß√£o e um terceiro mantendo o sistema dispon√≠vel.

A coordena√ß√£o √© feita automaticamente pelo ZooKeeper. Um dos _brokers_ ser√° eleito o l√≠der e ser√° respons√°vel por receber e entregar os dados. Os demais atuar√£o como r√©plicas, mantendo os dados sincronizados (_in-sync replica_ ou ISR). Essa gest√£o √© feita para cada parti√ß√£o  em cada t√≥pico, portanto a carga pode ser balanceada adequadamente.

Al√©m de gerenciar os _brokers_ em um _cluster_ e coordenar a lideran√ßa de parti√ß√£o, o ZooKeeper tamb√©m notifica os _brokers_ sobre mudan√ßas na estrutura, mantendo os metadados atualizados em todos os servidores. Com tantas atribui√ß√µes, o ZooKeeper √© obrigat√≥rio, mesmo que haja somente um _broker_.

O pr√≥prio ZooKeeper deve ser preferencialmente mantido em um _cluster_, sempre com n√∫mero √≠mpar de servidores. Eles elegem um l√≠der, que trata as entradas (_write_), com os demais seguidores efetuando as sa√≠das (_read_).

O ZooKeeper √© transparente aos consumidores e produtores, e acessado somente pelo Kafka.

#### Dimensionamento

Algumas dicas de boas pr√°ticas coletadas pela web apontam:

- N√£o ter mais do que algo entre 2.000 e 4.000 parti√ß√µes (independente dos t√≥picos) por _broker_;
- N√£o ter mais do que 20.000 parti√ß√µes no geral do _cluster_;
- Usar fator de replica√ß√£o entre 2 e 4, com 3 sendo o mais recomendado;
- Equilibrar o n√∫mero de parti√ß√µes de acordo com o n√∫mero de _brokers_ dispon√≠veis, algo entre 2x para um n√∫mero pequeno de brokers (< 6), ou 1x para um grande n√∫mero de _brokers_ (> 12). Lembre que esse n√∫mero deve ser **imut√°vel** para garantir a ordena√ß√£o por chaves. √â melhor grande do que pequeno, mas √© necess√°rio avaliar o desempenho.

## Modelo de armazenamento

Podemos pensar no Kafka como um grande _log_, onde dados em fluxo s√£o armazenados em uma sequ√™ncia temporal imut√°vel, para serem consumidos ordenadamente. Dados de mesma natureza s√£o agrupados em _t√≥picos_, e os t√≥picos s√£o gravados em arquivos f√≠sicos distribu√≠dos entre os _brokers_ chamados _parti√ß√µes_.

Os dados s√£o retidos por um tempo finito no Kafka (ex. 1 semana), portanto n√£o s√£o indefinidamente persistentes. Considere casos de uso de dados em movimento, e n√£o de dados em repouso.

### T√≥picos

Os t√≥picos s√£o agrupamentos de dados de mesma categoria. Atuam como tabelas em um banco relacional, por√©m sem as _constraints_. Outra diferen√ßa importante √© a impossibilidade de altera√ß√£o dos dados: os dados s√£o imut√°veis. Podem-se criar quantos t√≥picos forem necess√°rios, e cada t√≥pico pode receber dados de m√∫ltiplas origens e entregar dados para m√∫ltiplos destinos.

Ao criar um t√≥pico definimos um nome identificador, a quantidade de parti√ß√µes desejadas, e a quantidade de r√©plicas que estar√£o dispon√≠veis.

Ao se produzir uma mensagem (ou seja, gravar um mensagem em um t√≥pico) o Kafka verifica se o t√≥pico existe (cria com a configura√ß√£o padr√£o se n√£o existir), l√™ os seus metadados com as parti√ß√µes e configura√ß√µes de replica√ß√£o, e efetua a grava√ß√£o. Ap√≥s a grava√ß√£o, a mensagem estar√° dispon√≠vel para todos os consumidores interessados no t√≥pico.

üê±‚Äçüë§ [Discuss√£o sobre padr√µes de nomenclatura de t√≥picos](https://cnr.sh/essays/how-paint-bike-shed-kafka-topic-naming-conventions).

### Parti√ß√µes

Os dados de um t√≥pico s√£o gravados em arquivos f√≠sicos de dados, chamados parti√ß√µes. As parti√ß√µes s√£o numeradas sequencialmente, a partir de `0`. Ter mais de uma parti√ß√£o permite que os dados e a carga sejam distribu√≠dos, aumentando a toler√¢ncia a falhas e a disponibilidade.

Caso estejamos em um _cluster_, elas ser√£o distribu√≠das entre os _brokers_ dispon√≠veis, a crit√©rio do Kafka.

Exemplo:

Em um _cluster_ com 3 _brokers_ `1`, `2` e `3`, s√£o criados os t√≥picos `A` com 3 parti√ß√µes, `B` com 4 parti√ß√µes, `C` com 2 parti√ß√µes, e `D` com 1 parti√ß√£o.

- O t√≥pico `A` ter√° suas parti√ß√µes divididas igualmente entre os _brokers_, possivelmente um em cada;
- O t√≥pico `B` ter√° suas parti√ß√µes divididas igualmente entre os _brokers_ com um _broker_ que recebendo mais de uma parti√ß√£o desse t√≥pico;
- O t√≥pico `C` ter√° suas parti√ß√µes divididas entre os _brokers_, com algum _broker_ n√£o recebendo nenhuma parti√ß√£o;
- O t√≥pico `D` ter√° sua √∫nica parti√ß√£o alocada em um √∫nico _broker_.

Uma poss√≠vel configura√ß√£o final seria:

- _Broker_ `1`
  - T√≥pico `A` Parti√ß√£o `1`
  - T√≥pico `B` Parti√ß√£o `2`
  - T√≥pico `C` Parti√ß√£o `0`
  - T√≥pico `D` Parti√ß√£o `0`
- _Broker_ `2`
  - T√≥pico `A` Parti√ß√£o `0`
  - T√≥pico `B` Parti√ß√£o `1`
  - T√≥pico `B` Parti√ß√£o `3`
- _Broker_ `3`
  - T√≥pico `A` Parti√ß√£o `2`
  - T√≥pico `B` Parti√ß√£o `0`
  - T√≥pico `C` Parti√ß√£o `1`

Em uma vis√£o por t√≥pico:

- T√≥pico `A`
  - Parti√ß√£o `0` no _Broker_ `2`
  - Parti√ß√£o `1` no _Broker_ `1`
  - Parti√ß√£o `2` no _Broker_ `3`
- T√≥pico `B`
  - Parti√ß√£o `0` no _Broker_ `3`
  - Parti√ß√£o `1` no _Broker_ `2`
  - Parti√ß√£o `2` no _Broker_ `1`
  - Parti√ß√£o `3` no _Broker_ `2`
- T√≥pico `C`
  - Parti√ß√£o `0` no _Broker_ `1`
  - Parti√ß√£o `1` no _Broker_ `3`
- T√≥pico `D`
  - Parti√ß√£o `0` no _Broker_ `1`

### Replica√ß√£o

Em um t√≥pico criado com o fator de replica√ß√£o padr√£o `1`, cada parti√ß√£o cont√©m dados distintos, de forma que cada dado est√° em uma e somente uma parti√ß√£o. Se definirmos um n√∫mero maior de replica√ß√£o, haver√£o c√≥pias f√≠sicas da parti√ß√£o (chamadas _in-sync replicas_ ou _ISRs_) distribu√≠das necessariamente em _brokers_ diferentes, inativas e sincronizadas para assumir em caso de falha da parti√ß√£o ativa (chamada de parti√ß√£o l√≠der).

O fator de replica√ß√£o, portanto, √© definido entre 1 e a quantidade de _brokers_ existentes no _cluster_.

Sempre haver√° uma parti√ß√£o l√≠der eleita entre as r√©plicas, que atender√° toda a carga. As demais se manter√£o como c√≥pias est√°ticas sincronizadas, podendo assumir a lideran√ßa eventualmente a crit√©rio do Kafka.

üê±‚Äçüë§ Em caso de falha em todas as r√©plicas, o Kafka aguardar√° uma delas estar novamente online para eleg√™-la como l√≠der. Por padr√£o deve ser necessariamente uma r√©plica sincronizada (ISR). Pode-se mudar `unclean.leader.election=true` para permitir que seja uma r√©plica desatualizada. Isso melhora a disponibilidade ao pre√ßo de poss√≠veis perdas de algumas entradas. Bom para logs ou coleta de m√©tricas, onde algumas leituras podem ser perdidas em troca de um retorno mais r√°pido.

### Segmenta√ß√£o e reten√ß√£o

Os dados f√≠sicos uma parti√ß√£o s√£o armazenados em arquivo de dados `.log`, chamados de segmentos. Os dados s√£o indexados para facilitar a sua recupera√ß√£o (em tempo constante!) nos arquivos `.index`, e para agilizar o acesso por data de cria√ß√£o, nos arquivos `.timestamp`.

Os segmentos s√£o criados sequencialmente, de acordo com aas configura√ß√µes:

- `log.segment.bytes` (padr√£o. 1GB) indica o tamanho m√°ximo de um segmento.
- `log.segment.ms` (padr√£o 1 semana) indica o tempo a se aguardar at√© criar-se um novo segmento.

A escolha da pol√≠tica de segmenta√ß√£o deve observar a frequ√™ncia desejada para a cria√ß√£o de novos segmentos, observando a quantidade de arquivos a se manterem abertos (quanto √† mem√≥ria e limites do SO) e a frequ√™ncia de gatilhos de limpeza de log. Uma vez por dia pode ser um valor inicial aceit√°vel.

Os dados s√£o indexados para facilitar a sua recupera√ß√£o (em tempo constante!) nos arquivos `.index`, e para agilizar o acesso por data de cria√ß√£o, nos arquivos `.timestamp`.

Ap√≥s cada segmenta√ß√£o (ou seja, quando o segmento deixa de ser ao segmento ativo) √© realizado o processo de limpeza (ou compacta√ß√£o) de log. Nele s√£o eliminados registros antigos, de forma a manter o custo de armazenamento do Kafka controlado. O processo de limpeza ser√° iniciado a cada `log.cleaner.backoff.ms` (padr√£o 15s), e utilizar√° uma das duas pol√≠ticas dispon√≠veis:

Em `log.cleanup.policy=delete` (comportamento padr√£o), o log ser√° compactado e os registros eliminados de acordo com um prazo de expira√ß√£o ou do tamanho do segmento:

- `log.retention.hours` (padr√£o 1 semana) indica o tempo de reten√ß√£o. Todo dado mais velho do que esse tempo est√° sujeito a ser eliminado na compacta√ß√£o.
- `log.retention.bytes` (padr√£o `-1` = infinito) indica o tamanho m√°ximo de um segmento antes de estar sujeito a compacta√ß√£o.

Em `log.cleanup.policy=compact` temos a reten√ß√£o infinita dos registros, baseado na sua chave. Ao compactar mant√™m-se somente os registros mais recentes para cada chave, de forma que os consumidores ainda ter√£o o dado mais atualizado, apesar de n√£o ter mais acesso ao hist√≥rico.

Por exemplo, se produzirmos em um t√≥pico `salario`:

```
funcionario_1 => 2000
funcionario_3 => 2100
funcionario_4 => 4000
funcionario_2 => 3500
funcionario_1 => 2200
funcionario_3 => 2000
--- aqui foi realizada a √∫ltima compacta√ß√£o
funcionario_1 => 2500
funcionario_3 => 2200
funcionario_1 => 2505
```

A compacta√ß√£o processar√° os registros conforme a descri√ß√£o na lateral:

```
funcionario_1 => 2000 # esse registro ser√° exclu√≠do, pois h√° mais recente para essa chave
funcionario_3 => 2100 # esse registro ser√° exclu√≠do, pois h√° mais recente para essa chave
funcionario_4 => 4000 # esse registro ser√° mantido, pois √© o mais recente dessa chave
funcionario_2 => 3500 # esse registro ser√° mantido, pois √© o mais recente dessa chave
funcionario_1 => 2200 # esse registro ser√° exclu√≠do, pois h√° mais recente para essa chave
funcionario_3 => 2000 # esse registro ser√° exclu√≠do, pois h√° mais recente para essa chave
--- aqui foi realizada a √∫ltima compacta√ß√£o
funcionario_1 => 2500
funcionario_3 => 2200
funcionario_1 => 2505
```

Para um consumidor que esteja sempre online, nada ser√° percept√≠vel. Para um que inicie uma leitura do in√≠cio nesse ponto, ele receber√°:

```
funcionario_4 => 4000
funcionario_2 => 3500
funcionario_1 => 2500
funcionario_3 => 2200
funcionario_1 => 2505
```

Perceba que ele ainda pode receber mais de um registro por chave, j√° que as linhas mais recentes ainda n√£o foram compactadas pois est√£o no segmento ativo, mas ele tem garantido o registro mais recente de cada chave.

Esta configura√ß√£o est√° sujeita √†s seguintes configura√ß√µes:

- `segment.ms` (padr√£o 7d) tempo a aguardar antes de fechar um segmento;
- `segment.bytes` (padr√£o 1GB) tamanho m√°ximo do segmento;
- `min.compaction.lag.ms` (padr√£o 0) tempo aguardado antes que uma mensagem possa ser compactada;
- `delete.retention.ms` (padr√£o 24h) espera entre marca√ß√£o para dele√ß√£o e dele√ß√£o real;
- `min.cleanable.dirty.ratio` (padr√£o 0.5) efici√™ncia da compacta√ß√£o entre esfor√ßo e qualidade (mais baixo ocorre mais vezes).

### Dados, produtores e consumidores

Os s√£o mantidos nos t√≥picos, ap√≥s derem recebidos como _payloads_ da mensagens enviadas pelas aplica√ß√µes com o papel de produtores, e sob demanda entregues √†s aplica√ß√µes com o papel de consumidores.

Os dados s√£o armazenados e transportados em forma bin√°ria em _arrays_ de _bytes_. Assim, devemos tratar nos produtores e consumidores a serializa√ß√£o dos dados apropriada ao caso de uso. As bibliotecas costumam incluir diversos serializadores para formatos comuns.

#### Produ√ß√£o

Os produtores escrevem dados nos t√≥picos. N√£o √© necess√°rio saber qual a parti√ß√£o ou qual _broker_ acessar, mas somente o endere√ßo de um dos _brokers_ do _cluster_ e o nome do t√≥pico. Toda a resolu√ß√£o √© feita pelo Kafka, garantindo a estabilidade durante as indisponibilidades.

Os dados s√£o empacotados em registros ou mensagens contendo um cabe√ßalho, uma chave opcional e o valor do dado propriamente dito. S√£o enviados em lotes com um ou mais registros, com seu pr√≥prio cabe√ßalho, em um processo chamado _flush_.

O produtor pode solicitar tr√™s tipos de confirma√ß√£o de recebimento ap√≥s envio:

- `acks=0` n√£o aguarda confirma√ß√£o. √â mais r√°pido, mas n√£o garante a entrega.
- `acks=1` aguarda confirma√ß√£o do l√≠der, com poss√≠vel perda de dados em caso de falha no l√≠der entre a confirma√ß√£o e a replica√ß√£o.
- `acks=all` aguarda o l√≠der e a replica√ß√£o, portanto n√£o h√° perdas, ao custo de lat√™ncia.

üçå A confirma√ß√£o `acks=all` ainda assim pode tolerar alguma indisponibilidade nas r√©plicas. Isso pode ser ajustado pela combina√ß√£o do fator de replica√ß√£o do t√≥pico com a configura√ß√£o `min.insync.replicas` (quantidade m√≠nima de _brokers_ - inclu√≠ndo o l√≠der - que devem responder positivamente antes da confirma√ß√£o) no _broker_ ou no t√≥pico. Por exemplo, com fator de replica√ß√£o 5 e m√≠nimo _in sync_ de 3, a confirma√ß√£o vir√° mesmo com dois _brokers_ fora, apesar do `acks=all`.

O Kafka decide em qual parti√ß√£o o dado ser√° gravado. A quantidade de mensagens j√° gravadas n√£o influencia na decis√£o, portanto n√£o h√° divis√£o igualit√°ria de espa√ßo ocupado ou de quantidade de dados armazenados.

O dado recebe um identificador √∫nico incremental naquela parti√ß√£o, chamado _offset_, independente das demais parti√ß√µes. Ou seja, podemos ter o _offset_ `1` na parti√ß√£o `0` e tamb√©m na parti√ß√£o `1`, por√©m com dados diferentes sem nenhuma rela√ß√£o al√©m de estarem no mesmo t√≥pico. A ordem cronol√≥gica dos dados √© garantida dentro de uma parti√ß√£o, ou seja, o dado de _offset_ `17` certamente chegou antes do dado de _offset_ `18`. Por√©m, n√£o √© poss√≠vel garantir ordena√ß√£o entre diferentes parti√ß√µes, assim o dado de _offset_ `17` em uma parti√ß√£o pode ser anterior ao o dado de _offset_ `5` em outra.

Exemplo:

Foram enviados os dados üçå, ü•ë, üçâ, üçì e üçá ao t√≥pico `frutas` que possui duas parti√ß√µes, nessa sequ√™ncia. O Kafka decidiu armazenar da seguinte forma:

- Parti√ß√£o `0` = [üçâ, üçì]
  - Offset `1` = üçâ
  - Offset `2` = üçì
- Parti√ß√£o `1` = [üçå, ü•ë, üçá]
  - Offset `1` = üçå
  - Offset `2` = ü•ë
  - Offset `3` = üçá

Perceba que garantimos que üçì chegou ao t√≥pico ap√≥s üçâ, e que üçå chegou antes de üçá, mas nada podemos falar sobre a rela√ß√£o temporal entre üçâ e ü•ë.

Qualquer outra sequ√™ncia seria v√°lida, desde que os _offsets_ na mesma parti√ß√£o garantam a sequ√™ncia interna.

##### Retentativas e produtores idempotentes

Em caso de exce√ß√µes no envio, o erro pode ser tratado pelo desenvolvedor, ou automaticamente pelo produtor.

Os produtores podem usar a configura√ß√£o `retries` para fazer a retentativa autom√°tica, e esse √© inclusive o comportamento padr√£o do Kafka nas vers√µes >= 2.1. As configura√ß√µes importantes em rela√ß√£o a retentativas autom√°ticas s√£o:

- `retries` indica quantas tentativas ser√£o feitas em caso de exce√ß√£o (`0` = nenhuma);
- `retry.backoff.ms` indica o tempo entre as retentativas;
- `delivery.timeout.ms` indica o limite de tempo para retentativas (padr√£o √© 2 minutos);
- `max.in.flight.requests.per.connection` indica o n√∫mero m√°ximo de requisi√ß√µes em paralelo provenientes de uma mesma conex√£o. Caso seja maior do que um (padr√£o √© `5`), pode gerar grava√ß√µes fora de ordem em uma retentativa. O valor `1` garante o sequenciamento dentro da mesma conex√£o, mas bloqueia o paralelismo.

A solu√ß√£o mais simples para habilitar as retentativas e garantir a ordena√ß√£o dentro da conex√£o √© usar produtores idempotentes. Isso √© feito ativando a configura√ß√£o do produtor `enable.idempotence=true`. Isso exige `acks=all`, `max.in.flight.requests.per.connection=5` e `retries=Integer.MAX_VALUE` (ou seja, ou valores padr√£o). H√° uma explica√ß√£o detalhada do algoritmo usado [aqui](https://issues.apache.org/jira/browse/KAFKA-5494).

Em resumo, podemos criar um produtor seguro usando `enable.idempotence=true` associado a `min.insync.replicas=2` no t√≥pico ou no _broker_.

##### Desempenho

Algo que melhoria substancialmente o desempenho, contraintuitivamente, √© a uso de compacta√ß√£o na produ√ß√£o de mensagens, usando a configura√ß√£o `compression.type`. [Escolha um dos algoritmos de compacta√ß√£o](https://blog.cloudflare.com/squeezing-the-firehose/), por exemplo `lz4`, `snappy` ou `zstd` e os lotes de mensagens ser√£o compactados, reduzindo a lat√™ncia, o tr√°fego e o espa√ßo de armazenamento. Podemos controlar a forma√ß√£o dos lotes usando `linger.ms` (intervalo a aguardar antes de enviar, permitindo assim a forma√ß√£o de lotes maiores -- o padr√£o √© `0`) e `batch.size` (tamanho m√°ximo em bytes de um lote -- o padr√£o √© 16KB).

Uma boa configura√ß√£o para come√ßar a testar √© `compression.type=snappy`, `linger.ms=20` e `batch.size=32768` (32 * 1024 bytes = 32KB).

#### Consumo

Consumidores l√™em dados de t√≥picos. N√£o √© necess√°rio saber qual a parti√ß√£o ou qual _broker_ acessar, mas somente o endere√ßo de um dos _brokers_ do _cluster_ e o nome do t√≥pico. Toda a resolu√ß√£o √© feita pelo Kafka, garantindo a estabilidade durante as indisponibilidades.

Os dados ser√£o lidos ordenadamente dentro de cada parti√ß√£o, por√©m as parti√ß√µes ser√£o tratadas paralelamente. Ou seja, n√£o h√° garantia de entrega na ordem em que os dados chegaram no t√≥pico, mas somente dentro de cada parti√ß√£o.

Por exemplo, considerando o t√≥pico `frutas` no estado definido acima, um consumidor pode receber [üçå, ü•ë, üçâ, üçì, üçá] conforme a sequ√™ncia enviada, por√©m seria uma coincid√™ncia. S√£o igualmente v√°lidas e poss√≠vel quaisquer combina√ß√µes em que üçâ venha antes de üçì, e que üçá venha depois de ü•ë que por sua vez venha depois de üçå.

Ilustrando casos v√°lidos:

- [üçâ, üçì, üçå, ü•ë, üçá], em que se consumiu a parti√ß√£o `0` e depois a `1`;
- [üçå, ü•ë, üçá, üçâ, üçì], em que se consumiu a parti√ß√£o `1` e depois a `0`;
- [üçå, ü•ë, üçâ, üçì, üçá], em que se consumiu alternadamente, mantendo a sequencia original por coincid√™ncia;
- [üçå, ü•ë, üçâ, üçá, üçì], em que se consumiu alternadamente, n√£o mantendo a sequencia original mas mantendo a sequ√™ncia entre parti√ß√µes;
- [üçâ, üçå, ü•ë, üçì, üçá], como acima, mas em outra combina√ß√£o.

Os _offsets_ atuais de cada consumidor indicam o ponto atual de leitura de um consumidor em um t√≥pico, por parti√ß√£o, e permitem continuar do mesmo ponto ao retomar o consumo. Ficam armazenados no t√≥pico `__consumer_offsets` e s√£o mantidos automaticamente. Na entrega o consumidor ter√° o seu registro de _offsets_ atual alterado pelo Kafka, de forma que ele n√£o o receber√° em duplicidade, de acordo com a sem√¢ntica de entrega estabelecida.

O consumidor pode utilizar uma entre tr√™s sem√¢nticas de entrega:

- `at most once`: mensagens podem ser perdidas, mas nunca s√£o reenviadas em duplicidade. O _offset_ √© ajustado ao realizar a leitura, e em caso de erro no processamento das mensagens pelo consumidor, ao reiniciar as mensagem n√£o ser√£o reentregues, pois a confirma√ß√£o j√° foi dada.
- `at least once`: mensagens nunca s√£o perdidas, mas podem ser reenviadas em duplicidade. O _offset_ √© ajustado somente ao final do processo, e em caso de erro na altera√ß√£o do _offset_ ou erro no processamento pelo consumidor, a entrega pode reiniciar de onde come√ßou na √∫ltima vez, potencialmente repetindo dados j√° processados. Para implementar isso, fa√ßa o _commit_ de _offset_ manualmente, e garanta a idempot√™ncia do procedimento. √â o m√©todo preferido.
- `exactly once`: onde √© garantida a entrega uma e somente uma vez, por√©m √© restrita a processos internos do Kafka.

üí° Podemos implementar idempot√™ncia definindo chaves √∫nicas para cada registro recebido ao consumir. Caso o dado n√£o possua uma chave √∫nica intr√≠nseca, uma maneira bem simples √© utilizar uma chave no mecanismo de persist√™ncia que combine o nome do t√≥pico, a parti√ß√£o e o _offset_, combina√ß√£o essa √∫nica em uma instala√ß√£o Kafka.

O modelo de consumo √© o _poll_ (e n√£o _push_). Assim, em intervalos de tempo o consumidor requisita novos registros, em vez de ser estimulado pelo Kafka. Isso permite maior controle pelos consumidores da frequ√™ncia e do volume desejados para receber os dados.

üê±‚Äçüë§ Podemos especificar o tamanho m√≠nimo em bytes de um pacote a ser recebido usando `fetch.min.bytes` (o padr√£o √© `1`), criando lotes de transmiss√£o e reduzindo o tr√°fego ao custo da lat√™ncia (o m√°ximo √© controlado por `fetch.max.bytes` com o padr√£o de 50MB). Podemos tamb√©m controlar o tamanho m√°ximo do lote de registros usando `max.poll.records` (o padr√£o √© `500`), aumentando a quantidade em caso de mensagens pequenas ou de alta quantidade de RAM dispon√≠vel. O tamanho m√°ximo em bytes por parti√ß√£o pode ser controlado por `max.partitions.fetch.bytes` (o padr√£o √© 1MB). S√≥ altere essas configura√ß√µes em casos extremos de problemas de desempenho.

##### Reten√ß√£o e repeti√ß√£o

Espera-se que um consumidor (ou grupo de consumidores) fa√ßa leituras cont√≠nuas. Em caso de falha ou inatividade por um per√≠odo prolongado (maior do que o per√≠odo de reten√ß√£o) seu _offset_ se tornar√° inv√°lido ou descartado.

Nesses casos, h√° tr√™s op√ß√µes para o consumidor:

- com `auto.offset.reset=latest` ser√£o lidos somente os novos dados, a partir da retomada do consumo.
- com `auto.offset.reset=earliest` ser√£o lidos todos os dados dispon√≠veis novamente, desde o in√≠cio.
- com `auto.offset.reset=none` ser√° gerada uma exce√ß√£o caso n√£o haja nenhum _offset_.

Pode-se ajustar o tempo de reten√ß√£o por _broker_ usando `offset.retention.minutes` (o padr√£o √© uma semana).

Para repetir o consumo de um t√≥pico (receber novamente os dados a partir de um _offset_) em um grupo de consumidores, use `kafka-consumers-groups` com a op√ß√£o `--reset-offsets --execute --to-earliest` e reinicie os consumidores. Atente ao fato de que eles devem ser idempotentes.

Al√©m do _thread_ de _poll_ os consumidores em um grupo possuem um _thread_ de _heartbeat_ com o _broker_ coordenador do grupo. Ele serve para indicar que o consumidor ainda est√° ativo. Como os _threads_ s√£o independentes, a realiza√ß√£o de _polls_ muito espa√ßados pode gerar problemas.

- `session.timeout.ms` (padr√£o 10s) indica o tempo m√°ximo de espera entre _heartbeats_ antes que o consumidor seja considerado morto. Diminuir esse valor causar√° rebalanceamentos mais frequentes.
- `heartbeat.interval.ms` (padr√£o 3s) indica o tempo de espera entre envios de _heartbeats_ pelo consumidor. √â recomendado 1/3 do `timeout`.
- `max.poll.interval.ms` (padr√£o 5min) indica o m√°ximo tempo decorrido entre dois _polls_ antes de declarar o consumidor morto. Esse tempo deve ser ajustado caso o tempo de processamento seja muito alto e n√£o possa ser reduzido.

#### Chaves em mensagens

Em alguns casos de uso podemos necessitar de algum controle sobre o ordenamento das mensagens. Para isso, podemos enviar junto aos dados uma chave. O Kafka garante que dados enviados com a mesma chave ser√£o gravados na mesma parti√ß√£o, desde que o n√∫mero de parti√ß√µes se mantenha inalterado. Dessa maneira, temos a garantia do sequenciamento para mensagens com chaves semelhantes, j√° que estar√£o na mesma parti√ß√£o. Voc√™ ainda n√£o poder√° escolher em qual parti√ß√£o a primeira mensagem daquela chave ir√° ficar.

Um caso de uso comum seria o recebimento de posi√ß√µes GPS de diversos ve√≠culos onde queremos garantir as leituras na sequ√™ncia para cada um deles. Poder√≠amos obter essa garantia enviado o identificador do ve√≠culo na chave, por exemplo. Isso for√ßaria as leituras a ficarem na mesma parti√ß√£o, onde a ordem √© garantida.

As chaves, assim como os dados, s√£o armazenados e transportados em forma bin√°ria em _arrays_ de _bytes_, e podem ser serializados e desserializados pelos clientes conforme a necessidade.

ü§Ø A sele√ß√£o da parti√ß√£o √© feita atrav√©s do c√°lculo do resto da divis√£o de um inteiro calculado atrav√©s do [hash n√£o criptogr√°fico Murmur2](https://en.wikipedia.org/wiki/MurmurHash) do valor da chave pelo n√∫mero de parti√ß√µes dispon√≠veis. Isso claramente distribui as entradas de forma dependente da quantidade de parti√ß√µes, de forma que a altera√ß√£o nessa quantidade gera uma distribui√ß√£o diferente nas pr√≥ximas grava√ß√µes.

#### Grupos de consumidores

Para conseguirmos paralelizar o consumo sem repetir a leitura de um dado entre as inst√¢ncias consumidoras, precisamos criar uma afinidade entre elas. Fazemos isso criando grupos de consumidores, que nada mais s√£o do que indicadores de que eles compartilham o mesmo _offset_ em cada parti√ß√£o.

Um grupo de consumidores √© definido por um nome, e representa geralmente um _cluster_ de consumidores de uma √∫nica aplica√ß√£o.

Em um grupo, uma parti√ß√£o sempre ser√° lida pelo mesmo consumidor, garantindo a ordena√ß√£o. Essa coordena√ß√£o √© feita automaticamente pelo Kafka.

Caso hajam mais consumidores do que parti√ß√µes, eles ficar√£o inativos. Ainda assim podem ser √∫teis, pois ser√£o acionados assim que um dos consumidores fique indispon√≠vel.

<!-- ## Ecossistema

Kafka Connect API
  - Source Connectors
  - Sink Connectors
Kafka Streams API
Kafka Schema Registry
  - Apache Avro

Debezium - CDC

https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e -->